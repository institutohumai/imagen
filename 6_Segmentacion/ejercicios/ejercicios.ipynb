{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/CV/6_Segmentacion/ejercicios/ejercicios.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "2zJnTt_usuMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios Clase 6"
      ],
      "metadata": {
        "id": "-EQNZ8qbTkAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este notebook tendrán que implementar una nueva arquitectura de segmentación semántica llamada U-Net. Esta arquitectura, a diferencia de las Fully Convolutional vistas en clase, está diseñada para retener la información espacial que se pierde durante el downsampling. De esta manera, los feature maps generados durante el upsampling pueden tener un mejor contexto acerca de la posición de los píxeles originales."
      ],
      "metadata": {
        "id": "ad-HLylZTplX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seccion 1: Dataset"
      ],
      "metadata": {
        "id": "3CZ0LYd7WdeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1 \n",
        "\n",
        "Descargar el Dataset Pascal VOC2012 y realizarle image augmentation como se vio en clase. \n",
        "\n",
        "Nota: cambiar el crop size por (316,316) para que coincida con los shapes internos de U-Net"
      ],
      "metadata": {
        "id": "bDcqlw0BUbze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inserte su código aquí"
      ],
      "metadata": {
        "id": "TBEcPnxQt7ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sección 2: Modelo"
      ],
      "metadata": {
        "id": "A6F2642rWg9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo que usaremos en este notebook es la U-Net definida en el paper [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf).\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*YaLdptIoloK184uJQTH1HA.png)"
      ],
      "metadata": {
        "id": "GTg2WGsBWmLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura original tomaba como entrada una Imagen (572x572 según el paper) con dimensión de canal \"1\" para la imagen en escala de grises y generaba un mapa de segmentación de tamaño (388x388) con dimensión de canal equivalente a 2 que era la cantidad de clases que se entrenó para identificar. \n",
        "\n",
        "Sin embargo, en este notebook trabajaremos con imágenes a color y clasificaremos cada pixel en una de 21 clases por lo que los canales de entrada y salida deberán ser 3 y 21 respectivamente."
      ],
      "metadata": {
        "id": "OA67qiQTXPh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unet contiene principalmente tres partes:\n",
        "\n",
        "1. **El camino de contracción**: este es el lado descendente (lado izquierdo) de la \"U\", ayuda a obtener las features necesarias para la clasificación a medida que va reduciendo la dimensionalidad. .\n",
        "\n",
        "2. **El camino de expansión**: este es el lado ascendente (lado derecho) de la \"U\", ayuda a recuperar la dimensionalidad de las imágenes para poder hacer una clasificación a nivel de píxeles.\n",
        "\n",
        "3. **Conexiones de Salto**: las flechas grises largas desde el lado de contracción hasta el lado de expansión son las conexiones de salto. Estas se utilizan para \"retener la información espacial\" perdida durante la reducción de resolución de la imagen. De modo que, el mapa de características de la ruta de expansión pueda obtener un mejor contexto de la posición de los píxeles originales. Es decir las mismas features que se generaron al reducir la dimensionalidad son utilizadas para aumentarla."
      ],
      "metadata": {
        "id": "9ByiH1UCd5kN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación de la Arquitectura\n",
        "\n",
        "A continuación deberá implementar la arquitectura de U-Net en PyTorch"
      ],
      "metadata": {
        "id": "X8UYQBFogR-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque simple de doble convolución\n",
        "Una imagen de entrada se pasa a través de un par de convolución con tamaño de kernel de 3x3 y una activación de ReLU sobre ella. La dimensión del canal aumenta de “1” a “64”. Luego se pasa a través de otra convolución exactamente igual y otra ReLU, pero esta vez manteniendo la cantidad de canales. Por último, se aplica dropout con probabilidad 0.2\n",
        "\n",
        "![](https://miro.medium.com/max/640/1*Uan1yYCi3ZO1xrtLohyWzg.png)\n",
        "\n",
        "Recuerde que, en nuestro caso, los canales entrantes serán 3 porque son imágenes a color."
      ],
      "metadata": {
        "id": "jhhxcdA4giwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 2 \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6NH070wWY36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase SimpleConvolution para que imite el comportamiento del bloque simple de doble convolución"
      ],
      "metadata": {
        "id": "Md-ehAjynAet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel):\n",
        "        ## Complete la función init\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Complete la función forward"
      ],
      "metadata": {
        "id": "AfPtLpnPm-pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test SimpleConvolution\n",
        "block = SimpleConvolution(1,64)\n",
        "inp = torch.rand(4,1,572,572)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 64, 568, 568), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WZKEMX_v1r80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque Downsampling de Doble convolución\n",
        "\n",
        "Este bloque es exactamente igual al anterior sólo que antes de pasar las entradas por las capas convolucionales, se les aplica un max-pooling de kernel $2 \\times 2$. \n",
        "\n",
        "![](https://miro.medium.com/max/640/1*9zoULdYOeKQsLWQGExhVlQ.png)\n",
        "\n",
        "Este bloque se aplica 3 veces consecutivas en U-Net con diferentes cantidades de canales."
      ],
      "metadata": {
        "id": "BSVFankcpo5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 3\n"
      ],
      "metadata": {
        "id": "tezfEYW1rQQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase DownConvolution para que imite el comportamiento del bloque downsampling de doble convolución."
      ],
      "metadata": {
        "id": "in41pwsprTm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel):\n",
        "        ## Complete la función init\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Complete la función forward\n"
      ],
      "metadata": {
        "id": "i-TCmiCurQl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test DownConvolution\n",
        "block = DownConvolution(64,128)\n",
        "inp = torch.rand(4,64,568,568)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 128, 280, 280), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oLbZwumq2hWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque Upsampling de Doble Convolución"
      ],
      "metadata": {
        "id": "_dH4HWdfsPQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque es exactamente igual al bloque simple de doble convolución, solo que al final se añade una convolución transpuesta para hacer upsampling.\n",
        "\n",
        "Esta convolución transpuesta tiene un kernel de 2x2 y con un stride igual a 2. La dimensión del canal de salida en la convolución transpuesta se reduce a la mitad, ya que estaremos concatenando los feature maps provenientes del camino de contracción.\n",
        "\n",
        "![](https://miro.medium.com/max/640/1*nmfwdmaW5A7_zxI0BcPcGQ.png)"
      ],
      "metadata": {
        "id": "S4hGI2UCukjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 4\n"
      ],
      "metadata": {
        "id": "Ed4vij220zSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase UpConvolution para que imite el comportamiento del bloque upsampling de doble convolución."
      ],
      "metadata": {
        "id": "eXofnHsm0rWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UpConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel):\n",
        "      ## Complete la función init\n",
        "\n",
        "    def forward(self, x):\n",
        "      ## Complete la función forward"
      ],
      "metadata": {
        "id": "WxOy3PwltKYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test UpConvolution\n",
        "block = UpConvolution(512,256)\n",
        "inp = torch.rand(4,512,104,104)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 128, 200, 200), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jNvBSX0p3O0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque Final\n",
        "\n",
        "Este bloque es exactamente igual al bloque simple de doble convolución, solo que al final se añade una convolución $1 \\times 1$ para mapear los 64 canales que le llegan a la cantidad de clases deseadas.\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*cqs5XJRsBXS0RAkdIl_wUQ.png)\n",
        "\n",
        "Recuerda que nuestro dataset tiene 21 clases, no 2 como el paper original."
      ],
      "metadata": {
        "id": "Ybbs1KOM0CvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 5\n"
      ],
      "metadata": {
        "id": "P-ELOgyU06Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase LastConvolution para que imite el comportamiento del bloque final."
      ],
      "metadata": {
        "id": "l04xP19U1JcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LastConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel,num_classes):\n",
        "        ## Complete la función init\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Complete la función forward"
      ],
      "metadata": {
        "id": "pCzHtOPL0mSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test LastConvolution\n",
        "block = LastConvolution(128,64,2)\n",
        "inp = torch.rand(4,128,392,392)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 2, 388, 388), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xZMVJHZ73djJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conexiones de Salto"
      ],
      "metadata": {
        "id": "7_7KR8YO4Os-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al concatenar los features map del camino de contracción con los del camino de expansión, los primeros deben recortarse para que coincidan con la dimensión de los segundos.\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*2XyH7YGv7MuJWPycqx7hew.png)"
      ],
      "metadata": {
        "id": "wPrpuB-L4pCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 6"
      ],
      "metadata": {
        "id": "TyUCTQQR5epF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente la función crop_img para que el source_tensor se recorte a la dimensión del target_tensor."
      ],
      "metadata": {
        "id": "dNj2n_dc5f8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_img(source_tensor, target_tensor):\n",
        "    ## Complete la función "
      ],
      "metadata": {
        "id": "B-dCeeFpy9nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test crop_img\n",
        "src = torch.rand(4,128,280,280)\n",
        "target = torch.rand(4,256,200,200)\n",
        "crop_tensor = crop_img(src,target)\n",
        "assert crop_tensor.shape==(4, 128, 200, 200), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "itSwwL3N52WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo Completo"
      ],
      "metadata": {
        "id": "x-Aw6HYP89eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora deberá utilizar los bloques generados para ensamblar la U-Net\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*YaLdptIoloK184uJQTH1HA.png)"
      ],
      "metadata": {
        "id": "nfqY4etf9Hvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 7"
      ],
      "metadata": {
        "id": "iPzt8GvI9OLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente la clase UNet a partir de los bloques anteriores para que contenga a la red completa."
      ],
      "metadata": {
        "id": "H0SKMlzI9lue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes):\n",
        "        ## Complete la función init\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Complete la función init"
      ],
      "metadata": {
        "id": "oCDWk2KS867I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test UNet\n",
        "block = UNet(1, 2)\n",
        "inp = torch.rand(4, 1, 572, 572)\n",
        "out = block(inp)\n",
        "print(out.size())\n",
        "assert out.shape==(4, 2, 388, 388), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "id": "z-_r-yGI-Tzh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "DHjyYD9lBjMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota: debido a las limitaciones de RAM de GPU provistas por Colab, no podrá ejecutar el entrenamiento si ya ha corrido los tests. Los test crean variables que ocupan memoria y se termina acabando y rompiendo el kernel de Colab. Para poder ejecutar el entrenamiento, deberá reiniciar el entorno de ejecución y ejecutar las siguientes celdas sin haber ejecutado los tests.\n",
        "\n"
      ],
      "metadata": {
        "id": "TAAlxMqitLWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(inputs, targets):\n",
        "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
        "    \n",
        "def accuracy(y_hat, y):\n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = y_hat.argmax(axis=1)\n",
        "    cmp = y_hat.type(y.dtype) == y\n",
        "    return float(cmp.type(y.dtype).sum())"
      ],
      "metadata": {
        "id": "ppEb79i9AXpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr, wd, device = 5, 0.003, 1e-3, torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "unet = UNet(3, 21)\n",
        "trainer = torch.optim.SGD(unet.parameters(), lr=lr, weight_decay=wd)\n",
        "model = unet.to(device)"
      ],
      "metadata": {
        "id": "sw0wXPiwENE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    Acc = 0.0\n",
        "    Acc_N = 0\n",
        "    TestAcc = 0.0\n",
        "    TestN = 0\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = model(X)\n",
        "        y = crop_img(y.unsqueeze(1),y_hat)\n",
        "        y = y.squeeze(1)\n",
        "        l = loss(y_hat,y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "        Acc += accuracy(y_hat,y)\n",
        "        Acc_N += y.numel()\n",
        "    for X, y in test_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = model(X)\n",
        "        y = crop_img(y.unsqueeze(1),y_hat)\n",
        "        y = y.squeeze(1)\n",
        "        TestN += y.numel()\n",
        "        TestAcc += accuracy(y_hat, y)\n",
        "    print(f'epoch {epoch + 1}, loss {(L/N):f}\\\n",
        "          , train accuracy  {(Acc/Acc_N):f}, test accuracy {(TestAcc/TestN):f}')"
      ],
      "metadata": {
        "id": "xNB7Aw1K_bdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}