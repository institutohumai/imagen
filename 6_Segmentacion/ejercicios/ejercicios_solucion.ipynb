{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/CV/6_Segmentacion/ejercicios/ejercicios_solucion.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "2zJnTt_usuMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios Clase 6"
      ],
      "metadata": {
        "id": "-EQNZ8qbTkAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este notebook tendrán que implementar una nueva arquitectura de segmentación semántica llamada U-Net. Esta arquitectura, a diferencia de las Fully Convolutional vistas en clase, está diseñada para retener la información espacial que se pierde durante el downsampling. De esta manera, los feature maps generados durante el upsampling pueden tener un mejor contexto acerca de la posición de los píxeles originales."
      ],
      "metadata": {
        "id": "ad-HLylZTplX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seccion 1: Dataset"
      ],
      "metadata": {
        "id": "3CZ0LYd7WdeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1 \n",
        "\n",
        "Descargar el Dataset Pascal VOC2012 y realizarle image augmentation como se vio en clase. \n",
        "\n",
        "Nota: cambiar el crop size por (316,316) para que coincida con los shapes internos de U-Net"
      ],
      "metadata": {
        "id": "bDcqlw0BUbze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e4f4b79",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown Descarga de Datos\n",
        "import collections\n",
        "import hashlib\n",
        "import inspect\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import sys\n",
        "import tarfile\n",
        "import time\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import requests\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "\n",
        "def download(url, folder='data', sha1_hash=None):\n",
        "    \"\"\"Download a file to folder and return the local filepath.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    if not url.startswith('http'):\n",
        "        # For back compatability\n",
        "        url, sha1_hash = DATA_HUB[url]\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    fname = os.path.join(folder, url.split('/')[-1])\n",
        "    # Check if hit cache\n",
        "    if os.path.exists(fname) and sha1_hash:\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname\n",
        "    # Download\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def extract(filename, folder=None):\n",
        "    \"\"\"Extract a zip/tar file into folder.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    base_dir = os.path.dirname(filename)\n",
        "    _, ext = os.path.splitext(filename)\n",
        "    assert ext in ('.zip', '.tar', '.gz'), 'Only support zip/tar files.'\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(filename, 'r')\n",
        "    else:\n",
        "        fp = tarfile.open(filename, 'r')\n",
        "    if folder is None:\n",
        "        folder = base_dir\n",
        "    fp.extractall(folder)\n",
        "\n",
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
        "DATA_HUB['voc2012'] = (DATA_URL + 'VOCtrainval_11-May-2012.tar',\n",
        "                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')\n",
        "\n",
        "voc_dir = download_extract('voc2012', 'VOCdevkit/VOC2012')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9899e2a7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown Definición del Dataset\n",
        "def read_voc_images(voc_dir, n=-1,  is_train=True):\n",
        "    \"\"\"Read all VOC feature and label images.\"\"\"\n",
        "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
        "                             'train.txt' if is_train else 'val.txt')\n",
        "    mode = torchvision.io.image.ImageReadMode.RGB\n",
        "    with open(txt_fname, 'r') as f:\n",
        "        images = f.read().split()\n",
        "    features, labels = [], []\n",
        "    for i, fname in enumerate(images):\n",
        "        if i==n: break\n",
        "        features.append(torchvision.io.read_image(os.path.join(\n",
        "            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n",
        "        labels.append(torchvision.io.read_image(os.path.join(\n",
        "            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))\n",
        "        \n",
        "    return features, labels\n",
        "\n",
        "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
        "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
        "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
        "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
        "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
        "                [0, 64, 128]]\n",
        "\n",
        "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
        "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n",
        "\n",
        "def voc_colormap2label():\n",
        "    \"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\n",
        "    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
        "    for i, colormap in enumerate(VOC_COLORMAP):\n",
        "        colormap2label[\n",
        "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
        "    return colormap2label\n",
        "\n",
        "def voc_label_indices(colormap, colormap2label):\n",
        "    \"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\n",
        "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
        "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
        "           + colormap[:, :, 2])\n",
        "    return colormap2label[idx]\n",
        "\n",
        "def voc_rand_crop(feature, label, height, width):\n",
        "    \"\"\"Randomly crop both feature and label images.\"\"\"\n",
        "    rect = torchvision.transforms.RandomCrop.get_params(\n",
        "        feature, (height, width))\n",
        "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
        "    label = torchvision.transforms.functional.crop(label, *rect)\n",
        "    return feature, label\n",
        "\n",
        "class VOCSegDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, is_train, crop_size, voc_dir):\n",
        "        self.transform = torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        self.crop_size = crop_size\n",
        "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
        "        self.features = [self.normalize_image(feature)\n",
        "                         for feature in self.filter(features)]\n",
        "        self.labels = self.filter(labels)\n",
        "        self.colormap2label = voc_colormap2label()\n",
        "        print('read ' + str(len(self.features)) + ' examples')\n",
        "\n",
        "    def normalize_image(self, img):\n",
        "        return self.transform(img.float() / 255)\n",
        "\n",
        "    def filter(self, imgs):\n",
        "        return [img for img in imgs if (\n",
        "            img.shape[1] >= self.crop_size[0] and\n",
        "            img.shape[2] >= self.crop_size[1])]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
        "                                       *self.crop_size)\n",
        "        return (feature, voc_label_indices(label, self.colormap2label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5df5f968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e88e3c5-a0b9-4836-e5ff-d5d940ca672a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read 1380 examples\n",
            "read 1362 examples\n"
          ]
        }
      ],
      "source": [
        "#@markdown Creación del Dataset\n",
        "crop_size = (316, 316)\n",
        "voc_train = VOCSegDataset(True, crop_size, voc_dir)\n",
        "voc_test = VOCSegDataset(False, crop_size, voc_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d71ca620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "0cf8f30e-22a3-435d-b1db-f27c18d12bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "#@markdown Creación de los DataLoaders\n",
        "batch_size = 16\n",
        "train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,\n",
        "                                    drop_last=True,\n",
        "                                    num_workers=4)\n",
        "\n",
        "test_iter = torch.utils.data.DataLoader(voc_test, batch_size, shuffle=True,\n",
        "                                    drop_last=True,\n",
        "                                    num_workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sección 2: Modelo"
      ],
      "metadata": {
        "id": "A6F2642rWg9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo que usaremos en este notebook es la U-Net definida en el paper [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf).\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*YaLdptIoloK184uJQTH1HA.png)"
      ],
      "metadata": {
        "id": "GTg2WGsBWmLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura original tomaba como entrada una Imagen (572x572 según el paper) con dimensión de canal \"1\" para la imagen en escala de grises y generaba un mapa de segmentación de tamaño (388x388) con dimensión de canal equivalente a 2 que era la cantidad de clases que se entrenó para identificar. \n",
        "\n",
        "Sin embargo, en este notebook trabajaremos con imágenes a color y clasificaremos cada pixel en una de 21 clases por lo que los canales de entrada y salida deberán ser 3 y 21 respectivamente."
      ],
      "metadata": {
        "id": "OA67qiQTXPh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unet contiene principalmente tres partes:\n",
        "\n",
        "1. **El camino de contracción**: este es el lado descendente (lado izquierdo) de la \"U\", ayuda a obtener las features necesarias para la clasificación a medida que va reduciendo la dimensionalidad. .\n",
        "\n",
        "2. **El camino de expansión**: este es el lado ascendente (lado derecho) de la \"U\", ayuda a recuperar la dimensionalidad de las imágenes para poder hacer una clasificación a nivel de píxeles.\n",
        "\n",
        "3. **Conexiones de Salto**: las flechas grises largas desde el lado de contracción hasta el lado de expansión son las conexiones de salto. Estas se utilizan para \"retener la información espacial\" perdida durante la reducción de resolución de la imagen. De modo que, el mapa de características de la ruta de expansión pueda obtener un mejor contexto de la posición de los píxeles originales. Es decir las mismas features que se generaron al reducir la dimensionalidad son utilizadas para aumentarla."
      ],
      "metadata": {
        "id": "9ByiH1UCd5kN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación de la Arquitectura\n",
        "\n",
        "A continuación deberá implementar la arquitectura de U-Net en PyTorch"
      ],
      "metadata": {
        "id": "X8UYQBFogR-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque simple de doble convolución\n",
        "Una imagen de entrada se pasa a través de un par de convolución con tamaño de kernel de 3x3 y una activación de ReLU sobre ella. La dimensión del canal aumenta de “1” a “64”. Luego se pasa a través de otra convolución exactamente igual y otra ReLU, pero esta vez manteniendo la cantidad de canales. Por último, se aplica dropout con probabilidad 0.2\n",
        "\n",
        "![](https://miro.medium.com/max/640/1*Uan1yYCi3ZO1xrtLohyWzg.png)\n",
        "\n",
        "Recuerde que, en nuestro caso, los canales entrantes serán 3 porque son imágenes a color."
      ],
      "metadata": {
        "id": "jhhxcdA4giwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 2 \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6NH070wWY36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase SimpleConvolution para que imite el comportamiento del bloque simple de doble convolución"
      ],
      "metadata": {
        "id": "Md-ehAjynAet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel):\n",
        "        ## Complete la función init\n",
        "        super(SimpleConvolution, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channel,output_channel,(3,3))\n",
        "        self.conv2 = nn.Conv2d(output_channel,output_channel,(3,3))\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Complete la función forward\n",
        "        x = self.conv1(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "AfPtLpnPm-pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test SimpleConvolution\n",
        "block = SimpleConvolution(1,64)\n",
        "inp = torch.rand(4,1,572,572)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 64, 568, 568), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "WZKEMX_v1r80",
        "outputId": "a5ea7ed9-aa65-4c23-f7c4-ae086587a22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El test fue superado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque Downsampling de Doble convolución\n",
        "\n",
        "Este bloque es exactamente igual al anterior sólo que antes de pasar las entradas por las capas convolucionales, se les aplica un max-pooling de kernel $2 \\times 2$. \n",
        "\n",
        "![](https://miro.medium.com/max/640/1*9zoULdYOeKQsLWQGExhVlQ.png)\n",
        "\n",
        "Este bloque se aplica 3 veces consecutivas en U-Net con diferentes cantidades de canales."
      ],
      "metadata": {
        "id": "BSVFankcpo5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 3\n"
      ],
      "metadata": {
        "id": "tezfEYW1rQQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase DownConvolution para que imite el comportamiento del bloque downsampling de doble convolución."
      ],
      "metadata": {
        "id": "in41pwsprTm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel):\n",
        "        ## Complete la función init\n",
        "        super(DownConvolution, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channel,output_channel,(3,3))\n",
        "        self.conv2 = nn.Conv2d(output_channel,output_channel,(3,3))\n",
        "        self.maxpool = nn.MaxPool2d(2,2)\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Complete la función forward\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i-TCmiCurQl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test DownConvolution\n",
        "block = DownConvolution(64,128)\n",
        "inp = torch.rand(4,64,568,568)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 128, 280, 280), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "485fe0ca-aedf-45ec-e3a0-380a7696fdf2",
        "id": "oLbZwumq2hWE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El test fue superado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque Upsampling de Doble Convolución"
      ],
      "metadata": {
        "id": "_dH4HWdfsPQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque es exactamente igual al bloque simple de doble convolución, solo que al final se añade una convolución transpuesta para hacer upsampling.\n",
        "\n",
        "Esta convolución transpuesta tiene un kernel de 2x2 y con un stride igual a 2. La dimensión del canal de salida en la convolución transpuesta se reduce a la mitad, ya que estaremos concatenando los feature maps provenientes del camino de contracción.\n",
        "\n",
        "![](https://miro.medium.com/max/640/1*nmfwdmaW5A7_zxI0BcPcGQ.png)"
      ],
      "metadata": {
        "id": "S4hGI2UCukjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 4\n"
      ],
      "metadata": {
        "id": "Ed4vij220zSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase UpConvolution para que imite el comportamiento del bloque upsampling de doble convolución."
      ],
      "metadata": {
        "id": "eXofnHsm0rWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UpConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel):\n",
        "        super(UpConvolution, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channel,output_channel,(3,3))\n",
        "        self.conv2 = nn.Conv2d(output_channel,output_channel,(3,3))\n",
        "        self.convtranspose = nn.ConvTranspose2d(output_channel,output_channel//2,(2,2),(2,2))\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.convtranspose(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "WxOy3PwltKYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test UpConvolution\n",
        "block = UpConvolution(512,256)\n",
        "inp = torch.rand(4,512,104,104)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 128, 200, 200), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "16353a45-28ac-4c8a-c070-75282908fee6",
        "id": "jNvBSX0p3O0L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El test fue superado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bloque Final\n",
        "\n",
        "Este bloque es exactamente igual al bloque simple de doble convolución, solo que al final se añade una convolución $1 \\times 1$ para mapear los 64 canales que le llegan a la cantidad de clases deseadas.\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*cqs5XJRsBXS0RAkdIl_wUQ.png)\n",
        "\n",
        "Recuerda que nuestro dataset tiene 21 clases, no 2 como el paper original."
      ],
      "metadata": {
        "id": "Ybbs1KOM0CvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 5\n"
      ],
      "metadata": {
        "id": "P-ELOgyU06Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete la implementación de la clase LastConvolution para que imite el comportamiento del bloque final."
      ],
      "metadata": {
        "id": "l04xP19U1JcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LastConvolution(nn.Module):\n",
        "    def __init__(self,input_channel,output_channel,num_classes):\n",
        "        super(LastConvolution, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channel,output_channel,(3,3))\n",
        "        self.conv2 = nn.Conv2d(output_channel,output_channel,(3,3))\n",
        "        self.conv1d = nn.Conv2d(output_channel,num_classes,(1,1))\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv1d(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "pCzHtOPL0mSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test LastConvolution\n",
        "block = LastConvolution(128,64,2)\n",
        "inp = torch.rand(4,128,392,392)\n",
        "out = block(inp)\n",
        "assert out.shape==(4, 2, 388, 388), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "3d04d061-6669-48b2-89e7-152e8907aee4",
        "id": "xZMVJHZ73djJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El test fue superado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conexiones de Salto"
      ],
      "metadata": {
        "id": "7_7KR8YO4Os-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al concatenar los features map del camino de contracción con los del camino de expansión, los primeros deben recortarse para que coincidan con la dimensión de los segundos.\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*2XyH7YGv7MuJWPycqx7hew.png)"
      ],
      "metadata": {
        "id": "wPrpuB-L4pCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 6"
      ],
      "metadata": {
        "id": "TyUCTQQR5epF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente la función crop_img para que el source_tensor se recorte a la dimensión del target_tensor."
      ],
      "metadata": {
        "id": "dNj2n_dc5f8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_img(source_tensor, target_tensor):\n",
        "    source_tensor_size = source_tensor.size()[2]\n",
        "    target_tensor_size = target_tensor.size()[2]\n",
        "    diff = (source_tensor_size - target_tensor_size) // 2 # up and down strip --> so //2 takes only one side\n",
        "    return source_tensor[:,:,diff:-diff,diff:-diff]"
      ],
      "metadata": {
        "id": "B-dCeeFpy9nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test crop_img\n",
        "src = torch.rand(4,128,280,280)\n",
        "target = torch.rand(4,256,200,200)\n",
        "crop_tensor = crop_img(src,target)\n",
        "assert crop_tensor.shape==(4, 128, 200, 200), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "9e42addd-45dc-4e7e-a8ea-809c2597d1cf",
        "id": "itSwwL3N52WC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El test fue superado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo Completo"
      ],
      "metadata": {
        "id": "x-Aw6HYP89eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora deberá utilizar los bloques generados para ensamblar la U-Net\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*YaLdptIoloK184uJQTH1HA.png)"
      ],
      "metadata": {
        "id": "nfqY4etf9Hvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 7"
      ],
      "metadata": {
        "id": "iPzt8GvI9OLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente la clase UNet a partir de los bloques anteriores para que contenga a la red completa."
      ],
      "metadata": {
        "id": "H0SKMlzI9lue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.simpleConv = SimpleConvolution(input_channel, 64)\n",
        "        self.downConvBock1 = DownConvolution(64, 128)\n",
        "        self.downConvBock2 = DownConvolution(128, 256)\n",
        "        self.downConvBock3 = DownConvolution(256, 512)\n",
        "        self.midMaxpool = nn.MaxPool2d(2, 2)\n",
        "        self.upConvBlock0 = UpConvolution(512, 1024)\n",
        "        self.upConvBlock1 = UpConvolution(1024, 512)\n",
        "        self.upConvBlock2 = UpConvolution(512, 256)\n",
        "        self.upConvBlock3 = UpConvolution(256, 128)\n",
        "        self.lastConv = LastConvolution(128, 64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_1 = self.simpleConv(x)  # crop_x_1\n",
        "        x_2 = self.downConvBock1(x_1)  # crop_x_2\n",
        "        x_3 = self.downConvBock2(x_2)  # crop_x_3\n",
        "        x_4 = self.downConvBock3(x_3)  # crop_x_4\n",
        "        x_5 = self.midMaxpool(x_4)\n",
        "        x_6 = self.upConvBlock0(x_5)\n",
        "        crop_x_4 = crop_img(x_4, x_6)\n",
        "        concat_x_4_6 = torch.cat((crop_x_4, x_6), 1)\n",
        "        x_7 = self.upConvBlock1(concat_x_4_6)\n",
        "        crop_x_3 = crop_img(x_3, x_7)\n",
        "        concat_x_3_7 = torch.cat((crop_x_3, x_7), 1)\n",
        "        x_8 = self.upConvBlock2(concat_x_3_7)\n",
        "        crop_x_2 = crop_img(x_2, x_8)\n",
        "        concat_x_2_8 = torch.cat((crop_x_2, x_8), 1)\n",
        "        x_9 = self.upConvBlock3(concat_x_2_8)\n",
        "        crop_x_1 = crop_img(x_1, x_9)\n",
        "        concat_x_1_9 = torch.cat((crop_x_1, x_9), 1)\n",
        "        out = self.lastConv(concat_x_1_9)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "oCDWk2KS867I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test UNet\n",
        "block = UNet(1, 2)\n",
        "inp = torch.rand(4, 1, 572, 572)\n",
        "out = block(inp)\n",
        "print(out.size())\n",
        "assert out.shape==(4, 2, 388, 388), \"El test no fue superado\"\n",
        "print(\"El test fue superado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bebb17-4bf4-4f83-fcef-42fe128fc90b",
        "id": "z-_r-yGI-Tzh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2, 388, 388])\n",
            "El test fue superado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "DHjyYD9lBjMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota: debido a las limitaciones de RAM de GPU provistas por Colab, no podrá ejecutar el entrenamiento si ya ha corrido los tests. Los test crean variables que ocupan memoria y se termina acabando y rompiendo el kernel de Colab. Para poder ejecutar el entrenamiento, deberá reiniciar el entorno de ejecución y ejecutar las siguientes celdas sin haber ejecutado los tests.\n",
        "\n"
      ],
      "metadata": {
        "id": "TAAlxMqitLWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(inputs, targets):\n",
        "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
        "    \n",
        "def accuracy(y_hat, y):\n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = y_hat.argmax(axis=1)\n",
        "    cmp = y_hat.type(y.dtype) == y\n",
        "    return float(cmp.type(y.dtype).sum())"
      ],
      "metadata": {
        "id": "ppEb79i9AXpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr, wd, device = 5, 0.003, 1e-3, torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "unet = UNet(3, 21)\n",
        "trainer = torch.optim.SGD(unet.parameters(), lr=lr, weight_decay=wd)\n",
        "model = unet.to(device)"
      ],
      "metadata": {
        "id": "sw0wXPiwENE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    Acc = 0.0\n",
        "    Acc_N = 0\n",
        "    TestAcc = 0.0\n",
        "    TestN = 0\n",
        "    for X, y in train_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = model(X)\n",
        "        y = crop_img(y.unsqueeze(1),y_hat)\n",
        "        y = y.squeeze(1)\n",
        "        l = loss(y_hat,y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "        Acc += accuracy(y_hat,y)\n",
        "        Acc_N += y.numel()\n",
        "    for X, y in test_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = model(X)\n",
        "        y = crop_img(y.unsqueeze(1),y_hat)\n",
        "        y = y.squeeze(1)\n",
        "        TestN += y.numel()\n",
        "        TestAcc += accuracy(y_hat, y)\n",
        "    print(f'epoch {epoch + 1}, loss {(L/N):f}\\\n",
        "          , train accuracy  {(Acc/Acc_N):f}, test accuracy {(TestAcc/TestN):f}')"
      ],
      "metadata": {
        "id": "xNB7Aw1K_bdJ",
        "outputId": "3a0e4714-baf6-471a-8231-6b7c00e5dc17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 2.976859          , train accuracy  0.325101, test accuracy 0.496166\n",
            "epoch 2, loss 2.764970          , train accuracy  0.501332, test accuracy 0.499807\n",
            "epoch 3, loss 2.394452          , train accuracy  0.500350, test accuracy 0.492830\n",
            "epoch 4, loss 2.330368          , train accuracy  0.499674, test accuracy 0.491304\n",
            "epoch 5, loss 2.295999          , train accuracy  0.495698, test accuracy 0.491183\n",
            "epoch 6, loss 2.261164          , train accuracy  0.499677, test accuracy 0.500968\n",
            "epoch 7, loss 2.232270          , train accuracy  0.497774, test accuracy 0.496788\n",
            "epoch 8, loss 2.194594          , train accuracy  0.501336, test accuracy 0.498291\n",
            "epoch 9, loss 2.193033          , train accuracy  0.496406, test accuracy 0.500412\n",
            "epoch 10, loss 2.154083          , train accuracy  0.501587, test accuracy 0.500842\n"
          ]
        }
      ]
    }
  ]
}