{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ffece8d4311b4026bcc85f8f592e190f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e8ee46241223446ab03a3e35ddfb8d0c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5ebc2cc8da93481f8a7512669ff44896",
              "IPY_MODEL_99e9101337c64499b42926c50e762747"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "e8ee46241223446ab03a3e35ddfb8d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "5ebc2cc8da93481f8a7512669ff44896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e72fb94831e4bc59dbc7f3581ee18d2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4966400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4966400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a69bde48f604f36b7d5d8e2a59483b8"
          },
          "model_module_version": "1.5.0"
        },
        "99e9101337c64499b42926c50e762747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_948fb7c15e294ba5bc1e08fcd4033aa5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.74M/4.74M [00:00&lt;00:00, 28.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c05b9f8d16a460292c1a0efdf7750fd"
          },
          "model_module_version": "1.5.0"
        },
        "9e72fb94831e4bc59dbc7f3581ee18d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "3a69bde48f604f36b7d5d8e2a59483b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "948fb7c15e294ba5bc1e08fcd4033aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "0c05b9f8d16a460292c1a0efdf7750fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/CV/4_Transfer_Learning/ejercicios/ejercicios.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "zfdznMy4ECfO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "9XkI0vxSuzvZ"
      },
      "source": [
        "# Ejercicios Clase 4\n",
        "En este notebook implementaremos la técnica de transferencia de estilo de [\"Image Style Transfer Using Convolutional Neural Networks\" (Gatys et al., CVPR 2015)](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).\n",
        "\n",
        "La idea general es tomar dos imágenes y producir una nueva imagen que refleje el contenido de una pero el \"estilo\" artístico de la otra. Haremos esto formulando primero una función de pérdida que coincida con el contenido y el estilo de cada imagen respectiva en el espacio de características de una red profunda, y luego realizando un descenso de gradiente en los píxeles de la imagen en sí.\n",
        "\n",
        "La red profunda que usamos como extractor de características es SqueezeNet, un modelo pequeño que ha sido entrenado en ImageNet. Podríamos usar cualquier red, pero elegimos SqueezeNet aquí por su pequeño tamaño y eficiencia.\n",
        "\n",
        "Aquí hay un ejemplo de las imágenes que podrá producir al final de este notebook:\n",
        "\n",
        "![An image](https://i.imgur.com/ZV9CS3q.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "47A92LZluzvd"
      },
      "source": [
        "# Sección 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "CJs5nTkxuzvf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import PIL\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SQUEEZENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "SQUEEZENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "CHECKS_PATH = None\n",
        "!gdown https://drive.google.com/uc?id=1K94TyKQQhBt17M89ATiYHC1mK9p3fEav\n",
        "!gdown https://drive.google.com/uc?id=1Pcd9OhY_ujPGf63CQdC6gkpR8hqXqihw\n",
        "!unrar x styles.rar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "3t8hMI-zuzvg"
      },
      "source": [
        "CHECKS_PATH = 'style-transfer-checks.npz'\n",
        "\n",
        "assert CHECKS_PATH is not None, \"[!] Choose path to style-transfer-checks.npz\"\n",
        "\n",
        "STYLES_FOLDER = CHECKS_PATH.replace('style-transfer-checks.npz', 'styles')\n",
        "\n",
        "answers = dict(np.load(CHECKS_PATH))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "i-LQUboOuzvg"
      },
      "source": [
        "Le proporcionamos algunas funciones auxiliares para manejar imágenes, ya que para este TP estamos lidiando con archivos JPEG reales, no con datos preprocesados como con CIFAR-10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEbGlm-8NPyg"
      },
      "source": [
        "def preprocess(img, size=512):\n",
        "    \"\"\"Preprocesa una imagen PIL JPG para convertirlo en un tensor de Pytorch\n",
        "        que está listo para usarse como entrada en el modelo de CNN.\n",
        "        Pasos de preprocesamiento:\n",
        "            1) Cambiar el tamaño de la imagen (conservando la relación de aspecto) hasta que el lado más corto tenga la longitud \"size\".\n",
        "            2) Convertir la imagen PIL en un tensor de Pytorch.\n",
        "            3) Normalizar la media de los valores de píxeles de la imagen para que sean la media y la desviación\n",
        "             estandar esperadas por SqueezeNet\n",
        "            4) Agregar una dimensión de lotes en la primera posición del tensor: es decir el tensor con forma\n",
        "                (H, W, C) se convertirá en -> (1, H, W, C).\n",
        "    \"\"\"\n",
        "    transform = T.Compose([\n",
        "        T.Resize(size),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
        "                    std=SQUEEZENET_STD.tolist()),\n",
        "        T.Lambda(lambda x: x[None]),\n",
        "    ])\n",
        "    return transform(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZhFH9N7PGHb"
      },
      "source": [
        "def deprocess(img):\n",
        "    \"\"\" \"Desprocesa\" un tensor de Pytorch de la salida del modelo de CNN para convertirlo\n",
        "        una imagen PIL JPG que podemos mostrar, guardar, etc.\n",
        "        Pasos del desprocesamiento:\n",
        "            1) Eliminar la dimensión del lote en la primera posición accediendo al segmento en el índice 0.\n",
        "                 Un tensor de forma (1, H, W, C) se convertirá en -> (H, W, C).\n",
        "            2) Normalizar la desviación estándar: multiplicar cada canal del tensor de salida por 1 / s,\n",
        "                 antes de escalar por la desviación estándar de SqueezeNet. Sin cambios en la media.\n",
        "            3) Normalizar la media: restar la media (de ahí la -m) de cada canal del tensor de salida,\n",
        "                 centrando los elementos de nuevo antes de centrarse en la media de entrada de SqueezeNet.\n",
        "                 Sin cambios en el std dev.\n",
        "            4) Cambiar la escala de todos los valores en el tensor para que se encuentren en el intervalo [0, 1] para prepararse para\n",
        "                 transformándolo en valores de píxeles de imagen.\n",
        "            5) Convertir el tensor de Pytorch en una imagen PIL.\n",
        "    \"\"\"\n",
        "    transform = T.Compose([\n",
        "        T.Lambda(lambda x: x[0]),\n",
        "        T.Normalize(mean=[0, 0, 0], std=[1.0 / s for s in SQUEEZENET_STD.tolist()]),\n",
        "        T.Normalize(mean=[-m for m in SQUEEZENET_MEAN.tolist()], std=[1, 1, 1]),\n",
        "        T.Lambda(rescale),\n",
        "        T.ToPILImage(),\n",
        "    ])\n",
        "    return transform(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiPkWFgyP2DW"
      },
      "source": [
        "def rescale(x):\n",
        "    \"\"\" Una función utilizada internamente dentro de \"deprocess\".\n",
        "        Cambia la escala de los elementos de x linealmente para que estén en el intervalo [0, 1]\n",
        "        con los elementos mínimos asignados a 0, y los elementos máximos mapeados a 1.\n",
        "    \"\"\"\n",
        "    low, high = x.min(), x.max()\n",
        "    x_rescaled = (x - low) / (high - low)\n",
        "    return x_rescaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLT6omnxQboE"
      },
      "source": [
        "def rel_error(x,y):\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej6LwJlxQkbQ"
      },
      "source": [
        "def extract_features(x, cnn):\n",
        "    \"\"\"\n",
        "    Utiliza la CNN para extraer características de la imagen de entrada x.\n",
        "\n",
        "     Entradas:\n",
        "     - x: un tensor de PyTorch de forma (N, C, H, W) que contiene un minibatch de imágenes que\n",
        "       se enviará a la CNN.\n",
        "     - cnn: un modelo de PyTorch que usaremos para extraer características.\n",
        "\n",
        "     Salidas:\n",
        "     - features: una lista de características para las imágenes de entrada x extraídas usando el modelo cnn.\n",
        "       características [i] es un tensor de forma de PyTorch (N, C_i, H_i, W_i); recuerda que características\n",
        "       de diferentes capas de la red pueden tener diferentes números de canales (C_i) y\n",
        "       dimensiones espaciales (H_i, W_i).\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    prev_feat = x\n",
        "    for i, module in enumerate(cnn._modules.values()):\n",
        "        next_feat = module(prev_feat)\n",
        "        features.append(next_feat)\n",
        "        prev_feat = next_feat\n",
        "    return features\n",
        "\n",
        "#por favor ignore las advertencias sobre la inicialización\n",
        "def features_from_img(imgpath, imgsize, cnn):\n",
        "    img = preprocess(PIL.Image.open(imgpath), size=imgsize)\n",
        "    img_var = img.type(dtype)\n",
        "    return extract_features(img_var, cnn), img_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "6afOfDoYuzvh"
      },
      "source": [
        "Pytorch tiene dos tipos separados de tensores que contienen números de punto flotante: uno para operaciones en la CPU (torch.FloatTensor) y otro que usa CUDA para operaciones en la GPU (torch.cuda.FloatTensor). Usaremos esta variable de tipo más adelante, por lo que debemos establecer el tipo de tensor en uno de ellos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "zGKy4eQ_uzvh"
      },
      "source": [
        "dtype = torch.FloatTensor\n",
        "# ¡Descomente la siguiente línea si está en una máquina con una GPU configurada para PyTorch!\n",
        "#dtype = torch.cuda.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "ffece8d4311b4026bcc85f8f592e190f",
            "e8ee46241223446ab03a3e35ddfb8d0c",
            "5ebc2cc8da93481f8a7512669ff44896",
            "99e9101337c64499b42926c50e762747",
            "9e72fb94831e4bc59dbc7f3581ee18d2",
            "3a69bde48f604f36b7d5d8e2a59483b8",
            "948fb7c15e294ba5bc1e08fcd4033aa5",
            "0c05b9f8d16a460292c1a0efdf7750fd"
          ]
        },
        "id": "aT9gDRhkuzvi",
        "outputId": "cebef294-131b-43a5-bef5-85cd371b2a9e"
      },
      "source": [
        "# Carga el modelo SqueezeNet previamente entrenado.\n",
        "cnn = torchvision.models.squeezenet1_1(pretrained=True).features\n",
        "cnn.type(dtype)\n",
        "\n",
        "# No queremos entrenar más el modelo, por lo que no queremos que PyTorch \n",
        "# desperdicie recursos al calcular gradientes de parámetros que nunca actualizaremos.\n",
        "for param in cnn.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-f364aa15.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffece8d4311b4026bcc85f8f592e190f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4966400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tueT8TVeuzvi"
      },
      "source": [
        "# Sección 1: Cálculo de la Función de Pérdida\n",
        "\n",
        "Vamos a calcular ahora los tres componentes de nuestra función de pérdida. La función de pérdida es una suma ponderada de tres términos: pérdida de contenido + pérdida de estilo + pérdida total de variación. Vas a completar las funciones que calculan estos términos ponderados a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "ijl-mN73uzvj"
      },
      "source": [
        "## Sección 1A: Pérdida de Contenido\n",
        "\n",
        "Podemos generar una imagen que refleje el contenido de una imagen y el estilo de otra incorporando ambos en nuestra función de pérdida. Queremos penalizar las desviaciones del contenido con respecto a la imagen de contenido y las desviaciones del estilo con respecto a la imagen de estilo. Luego, podemos usar esta función de pérdida híbrida para realizar un descenso de gradiente no en los parámetros del modelo, sino en los valores de los píxeles de nuestra imagen original.\n",
        "\n",
        "Primero escribamos la función de pérdida de contenido. La pérdida de contenido mide cuánto difiere el mapa de activación de la imagen generada del mapa de activación de la imagen de origen. Solo nos importa la representación del contenido de una capa de la red (digamos, capa $\\ell$), que tiene mapas de activación $A^\\ell \\in \\mathbb{R}^{1 \\times C_\\ell \\times H_\\ell \\times W_\\ell}$. $C_\\ell$ es el número de filtros / canales en la capa $\\ell$, $H_\\ell$ and $W_\\ell$ son la altura y el ancho. Vamos a trabajar con versiones remodeladas de estos mapas de características que combinan todas las posiciones espaciales en una dimensión. Sea $F^\\ell \\in \\mathbb{R}^{C_\\ell \\times M_\\ell}$ el mapa de activación de la imagen actual y $P^\\ell \\in \\mathbb{R}^{C_\\ell \\times M_\\ell}$ mapa de activación para la imagen de origen del contenido, donde $M_\\ell=H_\\ell\\times W_\\ell$ es el número de elementos en cada mapa de activación. Cada fila de $F^\\ell$ or $P^\\ell$ representa las activaciones vectorizadas de un filtro particular, convolucionado sobre todas las posiciones de la imagen. Finalmente, sea $w_c$ el peso del término de pérdida de contenido en la función de pérdida.\n",
        "\n",
        "Entonces la pérdida de contenido viene dada por:\n",
        "\n",
        "$L_c = w_c \\times \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$\n",
        "\n",
        "Implemente la función `content_loss` en la siguiente celda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKOoiQPGrpcx"
      },
      "source": [
        "def content_loss(content_weight, content_current, content_original):\n",
        "    \"\"\"\n",
        "    Calcule la pérdida de contenido para la transferencia de estilo.\n",
        "\n",
        "    Entradas:\n",
        "    - content_weight: Escalar que da la ponderación de la pérdida de contenido.\n",
        "    - content_current: mapas de activación de la imagen actual; este es un tensor \n",
        "    de Pytorch de forma (1, C_l, H_l, W_l).\n",
        "    - content_target: mapas de activación de la imagen del contenido, tensor con forma (1, C_l, H_l, W_l).\n",
        "\n",
        "    Salidas:\n",
        "    - escalar que representa la pérdida de contenido \n",
        "    \"\"\"\n",
        "    # ***** INICIO DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) *****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # ***** FIN DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) ***** \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABXFPw4suzvk"
      },
      "source": [
        "Poné a prueba tu pérdida de contenido. Deberías ver errores inferiores a 0,001.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKsUGqJguzvk",
        "cellView": "form"
      },
      "source": [
        "# @markdown Test de pérdida de contenido\n",
        "def content_loss_test(correct):\n",
        "    content_image = '%s/tubingen.jpg' % (STYLES_FOLDER)\n",
        "    image_size =  192\n",
        "    content_layer = 3\n",
        "    content_weight = 6e-2\n",
        "    \n",
        "    c_feats, content_img_var = features_from_img(content_image, image_size, cnn)\n",
        "    \n",
        "    bad_img = torch.zeros(*content_img_var.data.size()).type(dtype)\n",
        "    feats = extract_features(bad_img, cnn)\n",
        "    \n",
        "    student_output = content_loss(content_weight, c_feats[content_layer], feats[content_layer]).cpu().data.numpy()\n",
        "    error = rel_error(correct, student_output)\n",
        "    print('El error máximo es {:.3f}'.format(error))\n",
        "\n",
        "content_loss_test(answers['cl_out'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sDeSVj9uzvl"
      },
      "source": [
        "## Sección 1B: Pérdida de Estilo\n",
        "\n",
        "Ahora podemos abordar la pérdida de estilo. Para una capa determinada $\\ell$, , la pérdida de estilo se define de la siguiente manera.\n",
        "\n",
        "Primero, calcule la matriz de Gram $G$ que representa las correlaciones entre los valores en cada canal del mapa de activación (es decir, las \"respuestas\" del filtro responsable de ese canal), donde $F$ se define de la misma forma que para la pérdida de contenido. La matriz de Gram es una aproximación de la matriz de covarianza: nos dice cómo los valores de cada canal (es decir, las activaciones de ese filtro) se correlacionan con los valores de todos los demás canales. Si tenemos $C$ canales, la matriz $G$ tendrá la forma $(C, C)$ para capturar estas correlaciones.\n",
        "\n",
        "Queremos que las estadísticas de activación de nuestra imagen generada coincidan con las estadísticas de activación de nuestra imagen de estilo, y hacer coincidir la covarianza (aproximada) es una forma de hacerlo. Hay varias formas de hacer esto, pero la matriz de Gram es buena porque es fácil de calcular y en la práctica muestra buenos resultados.\n",
        "\n",
        "Dado un mapa de características $F^\\ell$ con forma $(C_\\ell, H_\\ell, W_\\ell)$, podemos aplanar las dimensiones de alto y ancho para que se conviertan en solo 1 dimensión $M_\\ell = H_\\ell \\times W_\\ell$. Así la nueva forma de $F^\\ell$ es $(C_\\ell, M_\\ell)$. Entonces, la matriz de Gram tiene forma  $(C_\\ell, C_\\ell)$ donde cada elemento viene dado por la ecuación:\n",
        "\n",
        "$$G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}$$\n",
        "\n",
        "Suponiendo que $G^\\ell$ es la matriz de Gram del mapa de características de la imagen actual,  $A^\\ell$ es la Matriz de Gram del mapa de características de la imagen de estilo de origen y $w_\\ell$ un término de peso escalar, entonces la pérdida de estilo para la capa $\\ell$ es simplemente la distancia euclidiana ponderada entre las dos matrices de Gram:\n",
        "\n",
        "$$L_s^\\ell = w_\\ell \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
        "\n",
        "En la práctica, normalmente calculamos la pérdida de estilo en un conjunto de capas $\\mathcal{L}$ en lugar de solo una capa $\\ell$; entonces la pérdida total de estilo es la suma de las pérdidas de estilo en cada capa:\n",
        "\n",
        "$$L_s = \\sum_{\\ell \\in \\mathcal{L}} L_s^\\ell$$\n",
        "\n",
        "Comenzá implementando la función de cálculo de la matriz Gram `gram_matrix` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lIcsUN4vNv1"
      },
      "source": [
        "def gram_matrix(features, normalize=True):\n",
        "    \"\"\"\n",
        "    Calcula la matriz de Gram a partir de los mapas de activación.\n",
        "\n",
        "    Entradas:\n",
        "    - features: Tensor PyTorch de forma (N, C, H, W) que proporciona mapas de activación para\n",
        "      un lote de N imágenes.\n",
        "    - normalize: opcional para normalizar la matriz de Gram si es True,\n",
        "    divide a la matriz de Gram por el número de neuronas (H * W * C)\n",
        "\n",
        "    Salidas:\n",
        "    - gram: Tensor PyTorch de forma (N, C, C) que contiene las\n",
        "      (opcionalmente normalizadas) Matrices de Gram para las N imágenes de entrada.\n",
        "    \"\"\"\n",
        "    # ***** INICIO DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) *****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # ***** FIN DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) ***** \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDF-8hYruzvm"
      },
      "source": [
        "Poné a prueba tu Matriz de Gram. Deberías ver errores inferiores a 0,001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB0_2yU3uzvn",
        "cellView": "form"
      },
      "source": [
        "# @markdown Test de Matriz de Gram\n",
        "def gram_matrix_test(correct):\n",
        "    style_image = '%s/starry_night.jpg' % (STYLES_FOLDER)\n",
        "    style_size = 192\n",
        "    feats, _ = features_from_img(style_image, style_size, cnn)\n",
        "    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n",
        "    error = rel_error(correct, student_output)\n",
        "    print('Maximum error is {:.3f}'.format(error))\n",
        "    \n",
        "gram_matrix_test(answers['gm_out'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-okdUu8uzvn"
      },
      "source": [
        "A continuación, juntá todo e implementá la función de pérdida de estilo `style_loss` en la siguiente celda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuTRR8Klw-Ff"
      },
      "source": [
        "def style_loss(feats, style_layers, style_targets, style_weights):\n",
        "    \"\"\"\n",
        "    Calcula la pérdida de estilo en un conjunto de capas.\n",
        "\n",
        "    Entradas:\n",
        "    - feats: lista de los mapas de activación de la imagen actual generados por cada capa,\n",
        "    según lo producido por la función extract_features.\n",
        "    - style_layers: lista de índices de las capas para incluir en la pérdida de estilo.\n",
        "    - style_targets: lista de la misma longitud que style_layers, donde style_targets[i] es\n",
        "      un tensor de PyTorch que da la matriz de Gram de la imagen de estilo origen calculada\n",
        "      para la capa style_layers[i].\n",
        "    - style_weights: lista de la misma longitud que style_layers, donde style_weights[i]\n",
        "      es un escalar que da peso a la pérdida de estilo en la capa style_layers[i].\n",
        "\n",
        "    Salidas:\n",
        "    - style_loss: un tensor de PyTorch que contiene un escalar cuyo valor es la pérdida de estilo.\n",
        "    \"\"\"\n",
        "    # Sugerencia: puede hacer esto con un bucle for sobre las capas de estilo, y no \n",
        "    # debería ser mucho código (~ 5 líneas). Necesitará usar su función gram_matrix.\n",
        "    # ***** INICIO DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) *****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # ***** FIN DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) ***** "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm-QcVFtuzvo"
      },
      "source": [
        "Poné a prueba tu implementación de pérdida de estilo. El error debe ser inferior a 0,001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I44MMwcuzvo",
        "cellView": "form"
      },
      "source": [
        "# @markdown Test de pérdida de estilo\n",
        "def style_loss_test(correct):\n",
        "    content_image = '%s/tubingen.jpg' % (STYLES_FOLDER)\n",
        "    style_image = '%s/starry_night.jpg' % (STYLES_FOLDER)\n",
        "    image_size =  192\n",
        "    style_size = 192\n",
        "    style_layers = [1, 4, 6, 7]\n",
        "    style_weights = [300000, 1000, 15, 3]\n",
        "    \n",
        "    c_feats, _ = features_from_img(content_image, image_size, cnn)    \n",
        "    feats, _ = features_from_img(style_image, style_size, cnn)\n",
        "    style_targets = []\n",
        "    for idx in style_layers:\n",
        "        style_targets.append(gram_matrix(feats[idx].clone()))\n",
        "    \n",
        "    student_output = style_loss(c_feats, style_layers, style_targets, style_weights).cpu().data.numpy()\n",
        "    error = rel_error(correct, student_output)\n",
        "    print('El error es {:.3f}'.format(error))\n",
        "\n",
        "    \n",
        "style_loss_test(answers['sl_out'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbG06bvZuzvp"
      },
      "source": [
        "## Parte 1C: Regularización de variación total\n",
        "\n",
        "Resulta que, en la práctica, también es útil fomentar la suavidad en la imagen. Podemos hacer esto agregando otro término a nuestra pérdida que penalice los cambioas bruscos o la \"variación total\" en los valores de los píxeles.\n",
        "\n",
        "Puede calcular la \"variación total\" como la suma de los cuadrados de las diferencias en los valores de píxeles para todos los pares de píxeles que están uno al lado del otro (horizontal o verticalmente). Aquí sumamos la regularización de variación total para cada uno de los 3 canales de entrada (RGB) y ponderamos la pérdida total sumada por el peso de variación total,, $w_t$:\n",
        "\n",
        "$L_{tv} = w_t \\times \\left(\\sum_{c=1}^3\\sum_{i=1}^{H-1}\\sum_{j=1}^{W} (x_{i+1,j,c} - x_{i,j,c})^2 + \\sum_{c=1}^3\\sum_{i=1}^{H}\\sum_{j=1}^{W - 1} (x_{i,j+1,c} - x_{i,j,c})^2\\right)$\n",
        "\n",
        "Completá la definición del término de pérdida de TV en` tv_loss`. Para recibir el puntaje completo, tu implementación no debe tener ningún bucle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkFkXKX72170"
      },
      "source": [
        "def tv_loss(img, tv_weight):\n",
        "    \"\"\"\n",
        "    Calcule la pérdida de variación total.\n",
        "\n",
        "    Entradas:\n",
        "    - img: tensor PyTorch de forma (1, 3, H, W) que contenga una imagen de entrada.\n",
        "    - tv_weight: Escalar que contiene el peso w_t a utilizar para la pérdida de TV.\n",
        "\n",
        "    Salidas:\n",
        "    - loss: tensor PyTorch que contiene un escalar que da la pérdida de variación total\n",
        "      para img ponderado por tv_weight.\n",
        "    \"\"\"\n",
        "    # ¡Su implementación debería estar vectorizada y no requerir ningún bucle!\n",
        "    # ***** INICIO DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) *****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # ***** FIN DE SU CÓDIGO (NO BORRAR / MODIFICAR ESTA LÍNEA) ***** "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX0KGOSWuzvp"
      },
      "source": [
        "Poné a prueba tu implementación de la pérdida de TV. El error debe ser inferior a 0,001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebqXmawKuzvp",
        "cellView": "form"
      },
      "source": [
        "# @markdown Test de pérdida de variación total\n",
        "from inspect import getsourcelines\n",
        "import re\n",
        "\n",
        "def tv_loss_test(correct):\n",
        "    content_image = '%s/tubingen.jpg' % (STYLES_FOLDER)\n",
        "    image_size =  192\n",
        "    tv_weight = 2e-2\n",
        "\n",
        "    content_img = preprocess(PIL.Image.open(content_image), size=image_size).type(dtype)\n",
        "    \n",
        "    student_output = tv_loss(content_img, tv_weight).cpu().data.numpy()\n",
        "    error = rel_error(correct, student_output)\n",
        "    print('El error es {:.4f}'.format(error))\n",
        "    lines, _ = getsourcelines(tv_loss)\n",
        "    used_loop = any(bool(re.search(r\"for \\S* in\", line)) for line in lines)\n",
        "    if used_loop:\n",
        "        print(\"¡ADVERTENCIA! ¡Su implementación de tv_loss contiene un bucle! Para recibir el puntaje completo, tu implementación no debe tener ningún bucle\")\n",
        "    \n",
        "tv_loss_test(answers['tv_out'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaJBuCgeuzvq"
      },
      "source": [
        "# Sección 2: Transferencia de estilo\n",
        "\n",
        "Ahora estamos listos para encadenarlo todo (no deberías tener que modificar esta función):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "tags": [
          "pdf-ignore"
        ],
        "id": "thh828YEuzvq"
      },
      "source": [
        "def style_transfer(content_image, style_image, image_size, style_size, content_layer, content_weight,\n",
        "                   style_layers, style_weights, tv_weight, init_random = False):\n",
        "    \"\"\"\n",
        "    ¡Ejecuta la transferencia de estilo!\n",
        "\n",
        "    Entradas:\n",
        "    - content_image: nombre de archivo de la imagen de contenido\n",
        "    - style_image: nombre de archivo de la imagen de estilo\n",
        "    - image_size: tamaño de la dimensión de imagen más pequeña (utilizada para la pérdida de contenido y la imagen generada)\n",
        "    - style_size: tamaño de la dimensión de imagen de estilo más pequeña\n",
        "    - content_layer: capa que se utilizará para la pérdida de contenido\n",
        "    - content_weight: ponderación en la pérdida de contenido\n",
        "    - style_layers: lista de capas que se usarán para la pérdida de estilo\n",
        "    - style_weights: lista de pesos a usar para cada capa en style_layers\n",
        "    - tv_weight: peso del término de regularización de variación total\n",
        "    - init_random: inicializa la imagen inicial para un ruido aleatorio uniforme\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extrae los mapas de activación para la imagen de contenido\n",
        "    content_img = preprocess(PIL.Image.open(content_image), size=image_size).type(dtype)\n",
        "    feats = extract_features(content_img, cnn)\n",
        "    content_target = feats[content_layer].clone()\n",
        "\n",
        "    # Extrae los mapas de activación para la imagen de estilo\n",
        "    style_img = preprocess(PIL.Image.open(style_image), size=style_size).type(dtype)\n",
        "    feats = extract_features(style_img, cnn)\n",
        "    style_targets = []\n",
        "    for idx in style_layers:\n",
        "        style_targets.append(gram_matrix(feats[idx].clone()))\n",
        "\n",
        "    # Inicializa la imagen de salida a partir de la imagen de contenido o de ruido\n",
        "    if init_random:\n",
        "        img = torch.Tensor(content_img.size()).uniform_(0, 1).type(dtype)\n",
        "    else:\n",
        "        img = content_img.clone().type(dtype)\n",
        "\n",
        "    # ¡Queremos que el gradiente se calcule en nuestra imagen!\n",
        "    img.requires_grad_()\n",
        "    \n",
        "    # Configura hiperparámetros de optimización\n",
        "    initial_lr = 3.0\n",
        "    decayed_lr = 0.1\n",
        "    decay_lr_at = 180\n",
        "\n",
        "    # Tenga en cuenta que estamos optimizando los valores de los píxeles de la imagen\n",
        "    # pasando el tensor img cuya bandera require_grad está establecida en True \n",
        "    optimizer = torch.optim.Adam([img], lr=initial_lr)\n",
        "    \n",
        "    f, axarr = plt.subplots(1,2)\n",
        "    axarr[0].axis('off')\n",
        "    axarr[1].axis('off')\n",
        "    axarr[0].set_title('Content Source Img.')\n",
        "    axarr[1].set_title('Style Source Img.')\n",
        "    axarr[0].imshow(deprocess(content_img.cpu()))\n",
        "    axarr[1].imshow(deprocess(style_img.cpu()))\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "    \n",
        "    for t in range(200):\n",
        "        if t < 190:\n",
        "            img.data.clamp_(-1.5, 1.5)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        feats = extract_features(img, cnn)\n",
        "        \n",
        "        # Calcula la pérdida\n",
        "        c_loss = content_loss(content_weight, feats[content_layer], content_target)\n",
        "        s_loss = style_loss(feats, style_layers, style_targets, style_weights)\n",
        "        t_loss = tv_loss(img, tv_weight) \n",
        "        loss = c_loss + s_loss + t_loss\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        # Realiza descensos de gradiente en los pixeles de nuestra imagen\n",
        "        if t == decay_lr_at:\n",
        "            optimizer = torch.optim.Adam([img], lr=decayed_lr)\n",
        "        optimizer.step()\n",
        "\n",
        "        if t % 100 == 0:\n",
        "            print('Iteración {}'.format(t))\n",
        "            plt.axis('off')\n",
        "            plt.imshow(deprocess(img.data.cpu()))\n",
        "            plt.show()\n",
        "    print('Iteracion {}'.format(t))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(deprocess(img.data.cpu()))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BINW-Dq_uzvq"
      },
      "source": [
        "## ¡Generemos unas imágenes bonitas!\n",
        "\n",
        "Probá `style_transfer` en los tres conjuntos de parámetros establecidos a continuación. Asegurate de ejecutar las tres celdas. \n",
        "\n",
        "NO deberías modificar los parámetros de las siguientes tres celdas de código.\n",
        "\n",
        "Asegurate de adjuntar al notebook las imágenes obtenidas como resultado.\n",
        "\n",
        "Debajo de las siguientes tres celdas de código (en las que no deberías cambiar los hiperparámetros), sentite libre de copiar y pegar los parámetros para jugar con ellos y ver cómo cambia la imagen resultante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAhdPAvbuzvr"
      },
      "source": [
        "# Composition VII + Tubingen\n",
        "params1 = {\n",
        "    'content_image' : '%s/tubingen.jpg' % (STYLES_FOLDER),\n",
        "    'style_image' : '%s/composition_vii.jpg' % (STYLES_FOLDER),\n",
        "    'image_size' : 192,\n",
        "    'style_size' : 512,\n",
        "    'content_layer' : 3,\n",
        "    'content_weight' : 5e-2, \n",
        "    'style_layers' : (1, 4, 6, 7),\n",
        "    'style_weights' : (20000, 500, 12, 1),\n",
        "    'tv_weight' : 5e-2\n",
        "}\n",
        "\n",
        "style_transfer(**params1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkuAMCqIuzvr"
      },
      "source": [
        "# Scream + Tubingen\n",
        "params2 = {\n",
        "    'content_image':'%s/tubingen.jpg' % (STYLES_FOLDER),\n",
        "    'style_image':'%s/the_scream.jpg' % (STYLES_FOLDER),\n",
        "    'image_size':192,\n",
        "    'style_size':224,\n",
        "    'content_layer':3,\n",
        "    'content_weight':3e-2,\n",
        "    'style_layers':[1, 4, 6, 7],\n",
        "    'style_weights':[200000, 800, 12, 1],\n",
        "    'tv_weight':2e-2\n",
        "}\n",
        "\n",
        "style_transfer(**params2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSFLdk38uzvs"
      },
      "source": [
        "# Starry Night + Tubingen\n",
        "params3 = {\n",
        "    'content_image' : '%s/tubingen.jpg' % (STYLES_FOLDER),\n",
        "    'style_image' : '%s/starry_night.jpg' % (STYLES_FOLDER),\n",
        "    'image_size' : 192,\n",
        "    'style_size' : 192,\n",
        "    'content_layer' : 3,\n",
        "    'content_weight' : 6e-2,\n",
        "    'style_layers' : [1, 4, 6, 7],\n",
        "    'style_weights' : [300000, 1000, 15, 3],\n",
        "    'tv_weight' : 2e-2\n",
        "}\n",
        "\n",
        "style_transfer(**params3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaQUOgxf66YS"
      },
      "source": [
        "#Acá podés crear tu propio set de hiperparámetros y ver cómo afecta los resultados obtenidos."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "HuZExc4wuzvs"
      },
      "source": [
        "# Parte 3: Inversión de features\n",
        "\n",
        "El código que desarrollaste puede hacer otra cosa interesante. En un intento por comprender los tipos de features que las redes convolucionales aprenden a reconocer, un artículo reciente \"[Understanding Deep Image Representations by Inverting Them](https://arxiv.org/pdf/1412.0035.pdf)\" intenta reconstruir una imagen a partir de sus mapas de activación. Podemos implementar fácilmente esta idea utilizando los gradientes de la red pre-entrenada, que es exactamente lo que hicimos anteriormente (pero con dos sets de mapas de activación diferentes).\n",
        "\n",
        "Ahora, si configurás los pesos de estilo en 0 e inicializás la imagen de inicio con ruido aleatorio en lugar de usar la imagen de contenido, vas a reconstruir una imagen a partir de los mapas de activación de la imagen de contenido. Estás comenzando con ruido total, pero deberías terminar con algo que se parezca bastante a la imagen original.\n",
        "\n",
        "(De manera similar, podés hacer una \"síntesis de textura\" desde cero si establecés el peso del contenido en 0 e inicializás la imagen inicial con ruido aleatorio, pero no te vamos a pedir que lo hagas acá).\n",
        "\n",
        "Ejecutá la siguiente celda para probar la inversión de features.\n",
        "\n",
        "[1] Aravindh Mahendran, Andrea Vedaldi, \"Understanding Deep Image Representations by Inverting Them\", CVPR 2015\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6Nqf05tuzvs"
      },
      "source": [
        "# Feature Inversion -- Starry Night + Tubingen\n",
        "params_inv = {\n",
        "    'content_image' : '%s/tubingen.jpg' % (STYLES_FOLDER),\n",
        "    'style_image' : '%s/starry_night.jpg' % (STYLES_FOLDER),\n",
        "    'image_size' : 192,\n",
        "    'style_size' : 192,\n",
        "    'content_layer' : 3,\n",
        "    'content_weight' : 6e-2,\n",
        "    'style_layers' : [1, 4, 6, 7],\n",
        "    'style_weights' : [0, 0, 0, 0], # descartamos cualquier contribución del estilo a la pérdida\n",
        "    'tv_weight' : 2e-2, \n",
        "    'init_random': True # queremos inicializar nuestra imagen para que sea aleatoria\n",
        "}\n",
        "\n",
        "style_transfer(**params_inv)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}